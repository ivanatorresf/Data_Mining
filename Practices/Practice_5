 Support Vector Machine (SVM)
1.working directory
getwd()
setwd("home/marco/Escritorio/DataMining-master/MachineLearning/SVM")
getwd()
2.We import the data set in which we will work
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]

   We load the file with the dataset and we indicate that we only want to work with columns from 3 to 5.

3. Encoding the target feature as factor

dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))

4. Splitting the dataset into the Training set and Test set

library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)


    We load the caTools library, create the seed to randomize the data, divide the data 75% for training and 25% for testing, and create the training and testing subsets.

5. Feature Scaling

training_set[-3] = scale(training_set[-3])
test_set[-3] = scale(test_set[-3])

    scale is generic function whose default method centers and/or scales the columns of a numeric matrix.
    It is used to transform, give scala meaning to data, and help make the algorithm lighter; in this case we indicate that we want to apply the changes to all the columns except the prediction column.
==========================================================================================================================================================================================================
.install and import the e1071 library in order to use the SVM function.







7. Fitting SVM to the Training set

install.packages('e1071')
library(e1071)
classifier1 = svm(formula = Purchased ~ .,
                 data = training_set,
                 type = 'C-classification',
                 kernel = 'linear')

classifier2 = svm(formula = Purchased ~ .,
                 data = training_set,
                 type = 'C-classification',
                 kernel = 'polynomial')

classifier3 = svm(formula = Purchased ~ .,
                  data = training_set,
                  type = 'C-classification',
                  kernel = 'radial')

classifier4 = svm(formula = Purchased ~ .,
                  data = training_set,
                  type = 'C-classification',
                  kernel = 'sigmoid')

    It can be used to carry out general regression and classification, as well as density estimation. The following arguments are necessary:

        formula: The variable to predict (Purchased), and the characteristics to take as the basis for the prediction ('.' means that they will all be used).
        data: data set to use (training).
        type: svm can be used as a classification machine, as a regression machine, or for novelty detection. Depending of whether 'y' is a factor or not, the default setting for type is C-classification or eps-regression, respectively, but may be overwritten by setting an explicit value.
        kernel: The mathematical function used to do the data transformation in SVM is known as kernel, and there are 4 different types that can be applied, each with different algorithms and parameters:
            Linear: This is recommended when the linear separation of the data is easy.
            Polynomial.
            Radial Basis or Gaussian.
            Sigmoid.


8. Predicting the Test set results

y_pred1 = predict(classifier1, newdata = test_set[-3])
y_pred2 = predict(classifier2, newdata = test_set[-3])
y_pred3 = predict(classifier3, newdata = test_set[-3])
y_pred4 = predict(classifier4, newdata = test_set[-3])

    Using the predict function, the probabilities of the predictions to be made by the model are calculated, taking as reference the data of the classifiers created in the previous step, and indicating the data to be used  test_set minus column 3


9.Making the Confusion Matrix

cm1 = table(test_set[, 3], y_pred1)
cm2 = table(test_set[, 3], y_pred2)
cm3 = table(test_set[, 3], y_pred3)
cm4 = table(test_set[, 3], y_pred4)
cm1
cm2
cm3
cm4

10 Result



![Grafica](https://drive.google.com/open?id=1WPwTfoccHzY6qDyUNQJ_bPcQr0fBYcST)



11.Visualising the Training set results

library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
  We load the ElemStatLearn library to be able to visualize the training data.

y_grid1 = predict(classifier1, newdata = grid_set)
y_grid2 = predict(classifier2, newdata = grid_set)
y_grid3 = predict(classifier3, newdata = grid_set)
y_grid4 = predict(classifier4, newdata = grid_set)

    The predict function is used again to calculate the probabilities of the predictions, in this case taking the data from the grid created in the previous step (grid_set).

plot(set[, -3],
     main = 'SVM (Training set)',
     xlab = 'Age', ylab = 'Estimated Salary',
     xlim = range(X1), ylim = range(X2))

contour(X1, X2, matrix(as.numeric(y_grid1), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid1 == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))

    The plot function is used to graph based on the data in the first two columns of the set; the name of the graph is assigned and the names and limits of the axes.
    The contour function is used to add a contour line to the already created graph.
    The points function is used to add a figure to the graph, in this case a 0.01-inch rectangle per side colored 'green' if the specified condition is met, and if not, colored 'red'.

This graph corresponds to the type of linear kernel, in which it can be seen that the predictions are quite accurate, however there are still some errors. Compared with the results obtained from the rest of the kernels, and as it could be seen previously in the results of the confusion matrices, this type of kernel would come in second place due to its level of accuracy.



![Grafica11](https://drive.google.com/open?id=15mni0ejFZzGsjVPNkjtOefTZ0N-2vds_)



contour(X1, X2, matrix(as.numeric(y_grid2), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid2 == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))

 The resulting graph
 
 
 ![Grafica12](https://drive.google.com/open?id=1V1PcJPniR0WmdYxyo1kDwuQGbtdwDuq-)
 

contour(X1, X2, matrix(as.numeric(y_grid2), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid2 == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))

Another result



 ![Grafica13](https://drive.google.com/open?id=1KIRGKJwQbU2Q3RugxrKvITf4g1rM2iGO)
 



contour(X1, X2, matrix(as.numeric(y_grid3), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid3 == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))



![Grafica14](https://drive.google.com/open?id=1iQqf2cIeShSxMvWrXV-3vF7m4FNv39_Q)



12.Visualising the Test set results
library(ElemStatLearn)
set = test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')

y_grid1 = predict(classifier1, newdata = grid_set)
y_grid2 = predict(classifier2, newdata = grid_set)
y_grid3 = predict(classifier3, newdata = grid_set)
y_grid4 = predict(classifier4, newdata = grid_set)

plot(set[, -3], main = 'SVM (Test set)',
     xlab = 'Age', ylab = 'Estimated Salary',
     xlim = range(X1), ylim = range(X2))

contour(X1, X2, matrix(as.numeric(y_grid1), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid1 == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))



![Grafica15](https://drive.google.com/open?id=1G3bDEkUN1pRmbr46srQoF7NLyfMQzOnp)



contour(X1, X2, matrix(as.numeric(y_grid2), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid2 == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))



![Grafica16](https://drive.google.com/open?id=1gIU6tw97iUT_OITkMJOnaDA3wNi1ygvC)




contour(X1, X2, matrix(as.numeric(y_grid3), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid3 == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))



contour(X1, X2, matrix(as.numeric(y_grid4), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid4 == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))

![Grafica17](https://drive.google.com/open?id=1-kBk3jtdb2O4_YucbdtyQQ4UVWQN80hS)
