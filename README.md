# Mineria_de_Datos
Class Session
![Mineriade Datos](https://cdn.windowsreport.com/wp-content/uploads/2018/07/data-mining.png)


# Big-Data  

### <p align="center" > TECNOLÓGICO NACIONAL DE MÉXICO  INSTITUTO TECNOLÓGICO DE TIJUANA SUBDIRECCIÓN ACADÉMICA DEPARTAMENTO DE SISTEMAS Y COMPUTACIÓN DEPARTAMENTO DE SISTEMAS Y COMPUTACIÓN </p>
### <p align="center"> Enero - Junio 2020 </p>

###  <p align="center">  Ing. En Sistemas Computacionales. 	Datos Masivos (BDD-1704 IF9A	).</p>

### <p align="center">  Profesor: Jose Christian Romero Hernandez	</p>
### <p align="center">  Alumno: Ivan Adrian Torres Flores No.Ctrl.13210388  </p>
### <p align="center">  Alumno: Valenzuela Rosales Marco Asael No.Ctrl.16210585  </p>




# Unit 4

### &nbsp;&nbsp;Exam_1.

#### &nbsp;&nbsp;&nbsp;&nbsp; Instructions.
 
Develop the following problem with R and RStudio for knowledge extraction that the problem requires.
Implement the K-Means grouping model with the Iris.csv dataset that found at https://github.com/jcromerohdz/iris using the method kmeans () in R. Once the grouping model is obtained, do the analysis corresponding data display.
At the end of the development, explain in detail the model of K-Means group and what were their observations in visualization analysis of data.




#### &nbsp;&nbsp;&nbsp;&nbsp; Code.
getwd()
setwd("/home/marco/Escritorio/DataMining-master/Datasets")
getwd()


dataset <- read.csv('Iris.csv')
colnames(dataset) <- c("sepal_length", "sepal_width","petal_length","petal_width", "species")
dataset

dataset$species=NULL
dataset.scaled = as.data.frame(scale(dataset))

library(devtools)
devtools::install_github("kassambara/factoextra")

km <-kmeans(dataset.scaled,4)
km

aggregate(dataset.scaled, by = list(cluster = km$cluster),mean)

library(factoextra)

fviz_cluster(km, data = dataset.scaled)


```
In this R programming practice, we will make a brief introduction to two of the most well-known clustering or grouping models in Machine Learning, these are the K-means algorithm, and Clustering.
These are considered unsupervised classification models since (unlike supervised models where the input data has both components or variables and labels that each identify a particular class), in this case the classes are not known in advance. to which the data belongs, but it is the model's job to find similarities between them and thus group or classify them according to the intrinsic characteristics of their variables.
Have ggplot2 and Rtools installed before to work with these libraries.
In the first 3 lines we will again indicate the path where we have our dataframe file so that Rstudio can read it, in this case I have it stored in my documents, but I have to give it the exact address so that it can be read from the root that In this case I have it from my C disk following by users / adriantf / Documents.
In the next line we are going to load our dataset named iris.csv and we do it by writing dataset <- read.csv ('Iris.csv').
Now what we are doing is that we name the columns by writing colnames (dataset) <- c ("sepal_length", "sepal_width", "petal_length", "petal_width", "species") we verify by writing and executing dataset.
In the species column we will write it with dataset $ species = NULL to only have numerical variables.
Then we will make the scaled version with dataset.scaled = as.data.frame (scale (dataset)) of the original data of the dataset again, remember that we scale them so that the distances or each of the variables weighs equally so that they do not it finds biases in the fact that some distance counts much more than the rest.
Then we are going to load the devtools library from the devtools package, because we are going to install a githug package, we are going to use devtools :: install_github ("kassambara / factoextra") indicating the name of the user who is in this case kasambara and the library it's called factoextra.
This is a library that helps us to graphically represent the clustering generated by k-means in the event that we have many variables, in this case we notice that the dataset has 4 variables, and in this case the kassambrea-factoextra library helps this made using techniques that we have already seen that helps us visualize and extract the results of multi-variable data analysis.


The k-mean algorithm solves an optimization problem, the function to be optimized (minimized) being the sum of the quadratic distances of each object to the centroid of its cluster.
Clustering is a technique to find and classify K groups of data (clusters). Thus, the elements that contain similar characteristics are together in the same group, separated from the other groups with which they do not have specific characteristics.
To find out if the data is similar or different, the K-means algorithm uses the distance between the data. Observations that resemble a shorter distance between them In general, the Euclidean distance is used as a measure, although other functions can also be used.

We will do the kmeans starting by saving a km variable and we will use the kmeans function of the stat package where the only thing we have is to indicate it is the data set that we already have the scale and the number of divisions or groups that we will do in this case will 4 can be done by divisions or by centers in this case we will use the divisions to do it this is repeated and repeated and the maximum can be done 10 times as we see here
km <-kmeans (dataset.scaled, 4)
km

as we can see in the cluster separates them by groups we have what percentage of the information is summarized in the cluster this is the sum of the squares of the values ​​of each cluster with respect to the total if we calculate the sum of the squares of the values ​​of each cluster and the sum of object objects that belongs in each group over the total gives us the percentage

aggregate (dataset.scaled, by = list (cluster = km $ cluster), mean)
it is to add the data set already scaled that by the list given the equal cluster of the km that we already have the variable cluster and this we will extract the average and give us the average of our data set

the library is used to visualize the cluster
library (factoextra)

fviz_cluster (km, data = dataset.scaled)

we will create a new variable, with our km with our data set already scaled

we run it and it shows us the graphs we have in each graph a darker than serious point the center of each group we have 2 dimensions on the x axis we have that is the Dim one that has 72.6 percent of the data and of the second dimension we have which is 23.1 percent of the data
```

